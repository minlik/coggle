### 打卡汇总

| 任务名称                                         | 难度  | 所需技能             |
| :----------------------------------------------- | :---- | :------------------- |
| 任务1：报名比赛，下载比赛数据集并完成读取        | 低、1 | Pandas               |
| 任务2：对句子对提取TFIDF以及统计特征，训练和预测 | 高、2 | TDIDF                |
| 任务3：加载中文词向量，自己训练中文词向量        | 高、2 | gensim               |
| 任务4：使用中文词向量完成mean/max/sif句子编码    | 高、3 | mean/max/sif-pooling |
| 任务5：搭建SiamCNN/LSTM模型，训练和预测          | 高、3 | SiamCNN/SiamLSTM     |
| 任务6：搭建InferSent模型，训练和预测             | 高、3 | InferSent            |
| 任务7：搭建ESIM模型，训练和预测                  | 高、3 | ESIM                 |
| 任务8：使用BERT或ERNIE完成NSP任务                | 高、3 | BERT                 |
| 任务9：Bert-flow、Bert-white、SimCSE             | 高、3 | SimCSE               |

### 打卡要求

**注：**

- **需要所有的任务可以写在一个Notebook内**
- **推荐在打卡过程中加入思考过程，可以加入尝试&资料记录**
- **若使用Paddle进行打卡必须在百度 AI Studio平台运行，并设置公开**

**任务1：报名比赛，下载比赛数据集并完成读取**

- **步骤1** ：登录&报名比赛：https://aistudio.baidu.com/aistudio/competition/detail/45/0/task-definition
- **步骤2** ：下载比赛数据集
- **步骤3** ：使用`Pandas`完成数据读取。

**任务2：对句子对提取TFIDF以及统计特征，训练和预测**

- 参考代码：https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb

- 步骤1

   

  ：对句子对（句子A和句子B统计）如下特征：

  - 句子A包含的字符个数、句子B包含的字符个数
  - 句子A与句子B的编辑距离
  - 句子A与句子B共有单词的个数
  - 句子A与句子B共有字符的个数
  - 句子A与句子B共有单词的个数 / 句子A字符个数
  - 句子A与句子B共有单词的个数 / 句子B字符个数

- **步骤2** ：计算TFIDF，并对句子A和句子B进行特征转换

- **步骤3** ：计算句子A与句子B的TFIDF向量的内积距离

- **步骤4** ：将上述特征送入分类模型，训练并预测，将结果预测提交到比赛网站。

**任务3：加载中文词向量，自己训练中文词向量**

- **步骤1** ：使用`jieba`对中文句子进行分词
- **步骤2** ：使用[gensim中Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)训练分词后的句子，得到词向量。

**任务4：使用中文词向量完成mean/max/sif句子编码**

- **步骤1** ：单词通过word2vec编码为100维向量，则句子编码为N∗100的矩阵，N为句子单词个数。

- **步骤2** ：将N*100的矩阵进行`max-pooling`编码，转为100维度。

- **步骤3** ：将N*100的矩阵进行`mean-pooling`编码，转为100维度。

- **步骤4** ：将N*100的矩阵与单词的IDF进行矩阵相乘，即按照单词的词频进行加权，进行`tfidf-pooling`编码，转为100维度。

- **步骤5：**学习SIF编码的原理，进行sif编码，转为100维度。
- https://github.com/PrincetonML/SIF/blob/master/src/SIF_embedding.py#L30
  - https://openreview.net/pdf?id=SyK00v5xx

- **步骤6（可选）** ：通过上述步骤2-步骤5的编码，计算相似句子的相似度 vs 不相似句子的相似度， **绘制得到分布图，哪一种编码最优？**

**任务5：搭建SiamCNN/LSTM模型，训练和预测**

- **步骤1** ：将训练好的word2ve作为深度学习embeeding层的初始化参数。
- **步骤2** ：搭建SiamCNN（Word2Vec句子编码 + 1D CNN +FC）的孪生网络结构，完成训练和预测，提交测试集预测结果。
- **步骤3** ：搭建SiamLSTM（Word2Vec句子编码 + LSTM + FC）的孪生网络结构，完成训练和预测，提交测试集预测结果。

**任务6：搭建InferSent模型，训练和预测**

- **步骤1** ：将训练好的word2vex作为深度学习embeeding层的初始化参数。
- **步骤2** ：搭建InferSent模型，尝试不同的交叉方法。
- **步骤3** ：训练InferSent模型，提交测试集预测结果。

**任务7：搭建ESIM模型，训练和预测**

- **步骤1** ：将训练好的word2vex作为深度学习embeeding层的初始化参数。
- **步骤2** ：搭建ESIM模型，尝试不同的交叉方法。
- **步骤3** ：训练ESIM模型，提交测试集预测结果。

**任务8：使用BERT或ERNIE完成NSP任务**

- 参考代码：
  - https://aistudio.baidu.com/aistudio/projectdetail/3168859
  - [bert-nsp代码](https://gitee.com/coggle/competition-baseline/blob/master/competition/科大讯飞AI开发者大赛2021/中文问题相似度挑战赛/bert-nsp-xunfei.ipynb)
- **步骤1** ：学习Bert模型的使用。
- **步骤2** ：使用Bert完成NSP任务的训练和预测，提交测试集预测结果。

**任务8：Bert-flow、Bert-white、SimCSE（可选，不参与积分）**

- **步骤1** ：学习Bert-white原理和实现
- **步骤2** ：学习SimCSE原理和实现